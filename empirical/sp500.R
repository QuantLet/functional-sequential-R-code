# =====================================================
# 1. Clear and setup
# =====================================================
rm(list = ls())
if (requireNamespace("rstudioapi", quietly = TRUE)) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

library(readr)
library(dplyr)
library(lubridate)
library(fda)
library(refund)
library(MASS)

source("Method.R")   

# =====================================================
# 2. Data loading and functional data construction (SPX)
# =====================================================
spx_data <- read_csv("SPX.csv", show_col_types = FALSE)
spx_data <- spx_data %>%
  mutate(
    Date = as_date(DateTime),
    Year = year(Date)
  ) %>%
  filter(Year == 2020) %>%
  dplyr::select(Date, DateTime, Close)

daily_list <- spx_data %>%
  group_by(Date) %>%
  group_split()


 
# Calculate the log difference (rate of return) for each day
daily_list <- lapply(daily_list, function(day_data) {
  day_data <- day_data %>%
    mutate(Close= log(Close / lag(Close))) %>%  # Log difference to calculate rate of return
    filter(!is.na(Close))  # Remove NA values generated by lag function (for the first observation)
  
  return(day_data)
})

# Check the first few entries in daily_list to ensure it looks correct
print(head(daily_list[[1]]))


max_obs <- max(sapply(daily_list, nrow))
cat("Maximum observations per day:", max_obs, "\n")
min_obs <- min(sapply(daily_list, nrow))
nbasis <- min(21, floor(min_obs * 0.8))
cat("Using", nbasis, "basis functions\n")

basis <- create.bspline.basis(
  rangeval = c(0, 1),
  norder = 4,
  nbasis = nbasis
)

n_days <- length(daily_list)
coefs <- matrix(nrow = nbasis, ncol = n_days)
valid_days <- 0
day_names <- character()

for (i in seq_along(daily_list)) {
  day_data <- daily_list[[i]]
  n_points <- nrow(day_data)
  if (n_points < 10) next
  valid_days <- valid_days + 1
  day_names[valid_days] <- as.character(unique(day_data$Date))
  t_index <- seq(0, 1, length.out = n_points)
  y_vals <- day_data$Close
  fd_obj <- smooth.basis(argvals = t_index, y = y_vals, fdParobj = basis)
  coefs[, valid_days] <- fd_obj$fd$coefs
}
coefs <- coefs[, 1:valid_days, drop = FALSE]

fdata <- fd(
  coef = coefs,
  basisobj = basis,
  fdnames = list(
    "time" = paste0("t", seq(0, 100, length.out = 101)),
    "replicates" = day_names,
    "values" = "SPX_Close"
  )
)

cat("\nSuccessfully created functional data object with", valid_days, "days of SPX data.\n")
cat("Basis system:", basis$type, "with", basis$nbasis, "basis functions\n")

# =====================================================
# 3. Prepare for RSMS/CSMS/SSMS testing (SPX)
# =====================================================
n_days <- length(fdata$fdnames$replicates)

# Choose m/T.chan so m + m*T.chan <= n_days
T.chan <- 2
m_max <- floor(n_days / (1 + T.chan))
m <- min(100, m_max)
total_n <- m + m * T.chan

cat(sprintf("Using m = %d, T.chan = %d, total_n = %d (available days = %d)\n", m, T.chan, total_n, n_days))

t_grid <- seq(0, 1, length.out = 301)
values_all <- t(eval.fd(t_grid, fdata))[1:total_n, , drop = FALSE]
values_train <- values_all[1:m, , drop = FALSE]
values_monitor <- values_all[(m+1):total_n, , drop = FALSE]

Ly_train <- split(values_train, row(values_train))
Lt_train <- replicate(length(Ly_train), t_grid, simplify = FALSE)

fpca_train <- FPCA(
  Ly = Ly_train, Lt = Lt_train,
  optns = list(methodMuCovEst = 'smooth', FVEthreshold = 0.95, methodSelectK = 'FVE')
)


# Get the cumulative variance explained by the components
cumulative_variance <- fpca_train$cumFVE

# Set the desired threshold of variance to retain (e.g., 80%)
threshold <- 0.80

# Find the number of components that explain at least 80% of the variance
components_to_keep <- which(cumulative_variance >= threshold)[1]

# Output the number of components to keep
cat("Retaining", components_to_keep, "principal components for", threshold * 100, "% variance explained.\n")


mean_train <- fpca_train$mu
phi_train <- fpca_train$phi
scores_train <- fpca_train$xiEst
# Now, reduce the scores to the selected number of components
scores_train <-scores_train [, 1:components_to_keep]



centered_monitor <- sweep(values_monitor, 2, mean_train)
scores_monitor <- centered_monitor %*% phi_train * (1 / length(t_grid))
scores_monitor<-scores_monitor [, 1:components_to_keep]


scores_all <- rbind(scores_train, scores_monitor)

sample.omega <- var(scores_train)
A_estimate <- ldl(sample.omega)$lower
scores_whitened <- t(ginv(A_estimate) %*% t(scores_all))
scores_whitened <- scores_whitened[, 1:min(ncol(scores_whitened), 8), drop = FALSE]
if (is.vector(scores_whitened)) {
  scores_whitened <- matrix(scores_whitened, ncol = 1)
}

# =====================================================
# 4. Run RSMS, SSMS, CSMS
# =====================================================
alpha <- 0.05
gamma <- 0

cat("=== RSMS Test ===\n")
print(rsms.statistic.fpca.alt (scores_whitened, m = m, T.chan = T.chan, gamm = gamma, alpha = alpha))

cat("\n=== SSMS Test ===\n")
print(ssms.statistic.fpca.alt (scores_all, m = m, T.chan = T.chan, gamma = gamma, alpha = alpha))

cat("\n=== CSMS Test ===\n")
print(csms.statistic.fpca.alt (scores_all, m = m, T.chan = T.chan, gamm = gamma, alpha = alpha))

# =====================================================
# 5. Grid search (robust to not enough days!)
# =====================================================
all_dates <- fdata$fdnames$replicates
m_values <- c(50 )
T_values <- c(1, 2, 5)
gamma_values <- c(0, 0.15)
results <- list()
counter <- 1

for (mm in m_values) {
  for (TT in T_values) {
    total_len <- mm + mm * TT
    if (total_len > length(all_dates)) {
      cat(sprintf("Skipping m = %d, T = %d (need %d, only have %d days)\n", mm, TT, total_len, length(all_dates)))
      next
    }
    for (gamma in gamma_values) {
      cat(sprintf("\n===== m = %d, T = %d, gamma = %.2f =====\n", mm, TT, gamma))
      scores_subset <- scores_all[1:total_len, , drop = FALSE]
      scores_white_subset <- scores_whitened[1:total_len, , drop = FALSE]
      date_subset <- all_dates[1:total_len]
      rsms_res <- rsms.statistic.fpca.alt(scores_white_subset, m = mm, T.chan = TT, gamm = gamma, alpha = 0.05)
      ssms_res <- ssms.statistic.fpca.alt(scores_subset, m = mm, T.chan = TT, gamma = gamma, alpha = 0.05)
      csms_res <- csms.statistic.fpca.alt(scores_subset, m = mm, T.chan = TT, alpha = 0.05, gamma = gamma)
      rsms_date <- if (rsms_res$reject && !is.na(rsms_res$first_rejection)) date_subset[mm + rsms_res$first_rejection] else NA
      ssms_date <- if (ssms_res$reject && !is.na(ssms_res$first_rejection)) date_subset[mm + ssms_res$first_rejection] else NA
      csms_date <- if (csms_res$reject && !is.na(csms_res$first_rejection)) date_subset[mm + csms_res$first_rejection] else NA
      results[[counter]] <- list(
        m = mm, T.chan = TT, gamma = gamma,
        RSMS = c(rsms_res[1:3], first_rejection_date = rsms_date),
        SSMS = c(ssms_res[1:3], first_rejection_date = ssms_date),
        CSMS = c(csms_res[1:3], first_rejection_date = csms_date)
      )
      cat(sprintf("RSMS: stat = %.2f, crit = %.2f, reject = %s, first date = %s\n",
                  rsms_res$statistic, rsms_res$critical_value, rsms_res$reject, as.character(rsms_date)))
      cat(sprintf("SSMS: stat = %.2f, crit = %.2f, reject = %s, first date = %s\n",
                  ssms_res$statistic, ssms_res$critical_value, ssms_res$reject, as.character(ssms_date)))
      cat(sprintf("CSMS: stat = %.2f, crit = %.2f, reject = %s, first date = %s\n",
                  csms_res$statistic, csms_res$critical_value, csms_res$reject, as.character(csms_date)))
      counter <- counter + 1
    }
  }
}
